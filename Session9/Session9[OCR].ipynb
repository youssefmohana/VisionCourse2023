{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Optical Character Recognition (OCR)\n",
    "- Basics of OCR techniques\n",
    "- Text detection and extraction\n",
    "- OCR libraries and applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"OCR overview\"](https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/61154671c05e0cda312c86eb_optical-character-recognition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Image Preprocessing:\n",
    "- Grayscale Conversion: Convert the image to grayscale. This simplifies the image data to a single channel and can enhance text features.\n",
    "\n",
    "- Thresholding:\n",
    "    Apply thresholding to create a binary image, separating text from the background. This helps in distinguishing text more clearly.\n",
    "    \n",
    "- Noise Reduction:\n",
    "    Use techniques like blurring or morphological operations (erosion and dilation) to remove noise, enhancing the text's clarity.\n",
    "\n",
    "## 2. Text Detection:\n",
    "- Contour Detection:\n",
    "    Find contours in the processed image. Text regions often form distinct contours due to their contrast with the background.\n",
    "\n",
    "- Bounding Boxes:\n",
    "    Create bounding boxes around these detected text regions. These boxes enclose the text areas, enabling extraction.\n",
    "\n",
    "## 3. Text Extraction:\n",
    "- Extract Text Regions:\n",
    "    Crop the regions defined by the bounding boxes to focus only on the text areas.\n",
    "## 4. Optical Character Recognition:\n",
    "- Apply OCR:\n",
    "     Use pattern recognition algorithms or libraries like custom implementations of neural networks or more commonly, Tesseract OCR (as mentioned previously) to recognize text within the cropped regions.\n",
    "\n",
    "## 5. Steps FlowChart:\n",
    "![\"FlowChart OCR\"](https://miro.medium.com/v2/resize:fit:694/1*qBV12ANk-5epRv7231Zxzw.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "https://www.v7labs.com/blog/ocr-guide\n",
    "\n",
    "\n",
    "## install pytesseract\n",
    "\n",
    "https://medium.com/@BH_Chinmay/installation-of-tesseract-in-python-77daf712420f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytesseract in c:\\users\\tiba\\anaconda3\\envs\\opencv\\lib\\site-packages (0.3.10)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\tiba\\anaconda3\\envs\\opencv\\lib\\site-packages (from pytesseract) (23.2)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in c:\\users\\tiba\\anaconda3\\envs\\opencv\\lib\\site-packages (from pytesseract) (10.1.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the input image from disk, convert it to grayscale, and blur\n",
    "# it to reduce noise\n",
    "path = \"./imgs/logo.png\"\n",
    "image = cv2.imread(path)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "blurred = cv2.GaussianBlur(gray, (5, 5), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert\n",
      "\n",
      "R\n",
      "\n",
      "(C\n",
      "\n",
      "()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply Canny edge detection\n",
    "edges = cv2.Canny(blurred, 60, 150)  # Adjust thresholds for optimal edge detection\n",
    "\n",
    "# Find contours\n",
    "contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Draw edges and contours on the original image\n",
    "cv2.drawContours(image, contours, -1, (0, 255, 0), 2)  # Draw contours in green color\n",
    "image_with_edges = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)  # Convert edges to 3-channel for display\n",
    "result_image = cv2.hconcat([image, image_with_edges])  # Combine original image with edges\n",
    "\n",
    "# Extract text regions using bounding boxes\n",
    "for contour in contours:\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    cv2.rectangle(result_image, (x, y), (x + w, y + h), (255, 0, 0), 2)  # Draw bounding boxes in blue\n",
    "    text_region = image[y:y + h, x:x + w]\n",
    "\n",
    "    # Apply OCR on text regions\n",
    "    text = pytesseract.image_to_string(text_region, lang='eng', config='--psm 6')\n",
    "    print(text)\n",
    "\n",
    "# Show the resulting image with edges, contours, and bounding boxes\n",
    "cv2.imshow('Text Detection', result_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in deep learning detect text \n",
    "https://medium.com/technovators/scene-text-detection-in-python-with-east-and-craft-cbe03dda35d5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\layers\\concat_layer.cpp:109: error: (-201:Incorrect size of input array) Inconsistent shape for ConcatLayer in function 'cv::dnn::ConcatLayerImpl::getMemoryShapes'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m blob \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mblobFromImage(image, \u001b[38;5;241m1.0\u001b[39m, (W, H), (\u001b[38;5;241m123.68\u001b[39m, \u001b[38;5;241m116.78\u001b[39m, \u001b[38;5;241m103.94\u001b[39m), swapRB\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, crop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m net\u001b[38;5;241m.\u001b[39msetInput(blob)\n\u001b[1;32m---> 14\u001b[0m (scores, geometry) \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Set minimum confidence level and apply non-maxima suppression to get the text bounding boxes\u001b[39;00m\n\u001b[0;32m     17\u001b[0m min_confidence \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# Adjust this threshold for different images\u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\layers\\concat_layer.cpp:109: error: (-201:Incorrect size of input array) Inconsistent shape for ConcatLayer in function 'cv::dnn::ConcatLayerImpl::getMemoryShapes'\n"
     ]
    }
   ],
   "source": [
    "orig = image.copy()\n",
    "(H, W) = image.shape[:2]\n",
    "\n",
    "# Define the EAST text detector's parameters\n",
    "net = cv2.dnn.readNet(\"model/frozen_east_text_detection.pb\")  # Path to the pre-trained EAST text detector\n",
    "layer_names = [\n",
    "    \"feature_fusion/Conv_7/Sigmoid\",\n",
    "    \"feature_fusion/concat_3\"\n",
    "]\n",
    "\n",
    "# Prepare the image for EAST text detection\n",
    "blob = cv2.dnn.blobFromImage(image, 1.0, (W, H), (123.68, 116.78, 103.94), swapRB=True, crop=False)\n",
    "net.setInput(blob)\n",
    "(scores, geometry) = net.forward(layer_names)\n",
    "\n",
    "# Set minimum confidence level and apply non-maxima suppression to get the text bounding boxes\n",
    "min_confidence = 0.5  # Adjust this threshold for different images\n",
    "boxes = []\n",
    "for y in range(scores.shape[2]):\n",
    "    scores_data = scores[0, 0, y]\n",
    "    x_data0 = geometry[0, 0, y]\n",
    "    x_data1 = geometry[0, 1, y]\n",
    "    x_data2 = geometry[0, 2, y]\n",
    "    x_data3 = geometry[0, 3, y]\n",
    "    angles_data = geometry[0, 4, y]\n",
    "\n",
    "    for x in range(scores.shape[3]):\n",
    "        if scores_data[x] < min_confidence:\n",
    "            continue\n",
    "\n",
    "        # Calculate offset factor as the corresponding feature map point\n",
    "        (offsetX, offsetY) = (x * 4.0, y * 4.0)\n",
    "\n",
    "        # Extract the rotation angle for the prediction\n",
    "        angle = angles_data[x]\n",
    "\n",
    "        # Calculate the cosine and sine of the angle to get the rotation components\n",
    "        cos = np.cos(angle)\n",
    "        sin = np.sin(angle)\n",
    "\n",
    "        # Calculate the height and width of the bounding box\n",
    "        h = x_data0[x] + x_data2[x]\n",
    "        w = x_data1[x] + x_data3[x]\n",
    "\n",
    "        # Calculate the starting and ending (x, y)-coordinates for the text prediction bounding box\n",
    "        endX = int(offsetX + (cos * x_data1[x]) + (sin * x_data2[x]))\n",
    "        endY = int(offsetY - (sin * x_data1[x]) + (cos * x_data2[x]))\n",
    "        startX = int(endX - w)\n",
    "        startY = int(endY - h)\n",
    "\n",
    "        # Add the bounding box coordinates and rotation angle to the list\n",
    "        boxes.append((startX, startY, endX, endY, angle))\n",
    "\n",
    "# Apply non-maxima suppression to suppress weak, overlapping bounding boxes\n",
    "indices = cv2.dnn.NMSBoxesRotated(boxes, scores_data, min_confidence, 0.4)\n",
    "\n",
    "# Loop over the indices and extract text regions for OCR\n",
    "for i in indices:\n",
    "    i = i[0]\n",
    "    (startX, startY, endX, endY, angle) = boxes[i]\n",
    "    angle = np.degrees(angle)\n",
    "    angle = -angle if angle < -45 else 90 - angle\n",
    "\n",
    "    # Rotate the bounding box and extract the text region\n",
    "    center = ((startX + endX) // 2, (startY + endY) // 2)\n",
    "    size = (endX - startX, endY - startY)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated = cv2.warpAffine(orig, M, orig.shape[1::-1], flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "    cropped = cv2.getRectSubPix(rotated, size, center)\n",
    "\n",
    "    # Apply OCR on the text region using Tesseract\n",
    "    text = pytesseract.image_to_string(cropped)\n",
    "    print(\"Detected Text:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
