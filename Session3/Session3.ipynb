{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa7bdb83",
   "metadata": {},
   "source": [
    "# Agenda[Feature Extraction and Description] \n",
    "\n",
    "\n",
    "1. Feature detection algorithms (e.g., Harris, SIFT, SURF)\n",
    "2. Feature description and matching techniques\n",
    "3. Feature-based image alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209e33dc",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a56e5c",
   "metadata": {},
   "source": [
    "<img src=\"data:image/webp;base64,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\" alt=\"drawing\" style=\"width:200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ddf3c6",
   "metadata": {},
   "source": [
    "## Keypoint Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d89aa7",
   "metadata": {},
   "source": [
    "Have you ever wondered how the human brain is capable of recognizing many different objects so quickly? We can recognize patterns and shapes in a split of a second. For example, humans have a natural ability to recognize faces. New research shows that the baby’s brain recognizes faces from the earliest days. So, how can we do that? Well, the answer is simple. Because our brain is triggered by the most interesting points of an image. These interesting, distinct features we also call keypoints. In many computer vision and machine learning applications, we need these feature points that will assist us to compare and detect objects or scenes.\n",
    "\n",
    "<img src=\"https://media5.datahacker.rs/2020/09/Picture1-1536x416.jpg\" alt=\"drawing\" style=\"width:500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2112d816",
   "metadata": {},
   "source": [
    "## Where can we find keypoints in the image?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45351085",
   "metadata": {},
   "source": [
    "The next question that we need to answer is where we can locate these points in the image? Well, we can find them in regions where we can detect rapid pixel intensity change. These areas are edges and corners. The edge represents an interesting set of points that can help us to better characterize the object in the image. Corners are even more distinct because they are located at the intersection of two or more lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e72c1a1",
   "metadata": {},
   "source": [
    "<img src=\"https://media5.datahacker.rs/2020/09/23.jpg\" alt=\"drawing\" style=\"width:500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af320a41",
   "metadata": {},
   "source": [
    "If we have look at the above image we can differentiate two distinctive sets of points. The points located inside these geometric shapes are not that interesting because their small pixel neighborhood is completely identical. On the other hand, the edges of these objects are much more interesting because their local neighborhood is different. For example, if we look at the triangle in the following image we can see the pixels that create an edge are surrounded with pixels with different intensities. That means that the intensity of a pixel inside the triangle is different from the intensity of a pixel outside of a triangle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552759a2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8839480d",
   "metadata": {},
   "source": [
    "## Feature detection algorithms\n",
    "- Harris Corner Detector\n",
    "- Scale Invariant Feature Transform (SIFT)\n",
    "- ORB (Oriented FAST and Rotated BRIEF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaaf339",
   "metadata": {},
   "source": [
    "## Harris Corner Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833e6853",
   "metadata": {},
   "source": [
    "Now when we know that the most interesting parts of an image are corners, it is important to learn how to detect them. Among many algorithms, Harris Corner Detector is one of the most popular methods for feature detection due to its good results and robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd923d4",
   "metadata": {},
   "source": [
    "<img src=\"https://media5.datahacker.rs/2020/09/index2-768x566.png\" alt=\"drawing\" style=\"width:500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9676f33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed libraries \n",
    "import cv2 as cv \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6688d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread('images/chessboard.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5b0700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.imshow(\"Chessboard\", img)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1acf10b",
   "metadata": {},
   "source": [
    "### Harris Corner Detector [cornerHarris()](https://docs.opencv.org/3.4/dd/d1a/group__imgproc__feature.html#gac1fc3598018010880e370e2f709b4345) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ad7f14",
   "metadata": {},
   "source": [
    "This function consist of the following parameters:\n",
    "- `src`: Input image that should be `grayscale` and `float32` type.\n",
    "- `blockSize`: It is the size of the neighborhood considered for corner detection.\n",
    "- `ksize`:Aperture parameter of Sobel derivative used.\n",
    "- `k`:Harris detector free parameter in the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534bfe81",
   "metadata": {},
   "source": [
    "#### Convert image to Grayscale and float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df4227bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "gray = np.float32(gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ab9844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 11.,  11.,  11., ..., 255., 255., 255.],\n",
       "       [ 11.,  11.,  11., ..., 255., 255., 255.],\n",
       "       [ 11.,  11.,  11., ..., 255., 255., 255.],\n",
       "       ...,\n",
       "       [255., 255., 255., ...,  11.,  11.,  11.],\n",
       "       [255., 255., 255., ...,  11.,  11.,  11.],\n",
       "       [255., 255., 255., ...,  11.,  11.,  11.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ab8f03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24f489ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = cv.cornerHarris(gray, 2, 3, 0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fe04984",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.imshow(\"dst\", dst)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df61870c",
   "metadata": {},
   "source": [
    "Then, we will apply the process of dilation with the function [`cv2.dilate()`](https://docs.opencv.org/3.4/d4/d86/group__imgproc__filter.html#ga4ff0f3318642c4f469d0e11f242f3b6c). We use this function to increase the object area and to emphasize features. Finally, we need to apply a threshold for an optimal value that can vary depending on an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4eb7d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Applying dilation to increase the object area and to emphasize features\n",
    "dst = cv.dilate(dst, None)\n",
    "cv.imshow(\"dst\", dst)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2b5a68e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for an optimal value. It may vary depending on the image.\n",
    "img1 = img.copy()\n",
    "img1[dst>0.2*dst.max()]=[0, 0, 255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "69086290",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.imshow(\"Chessboard corners\", img1)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9388cb97",
   "metadata": {},
   "source": [
    "We can see that when we decrease a threshold value the number of detected points will increase. So, when a value for a threshold was set to `0.02 * dis.max` we detected just a few points that are the most dominant. That means that keypoints in the other three images are not as distinct as the points on the first image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35d0416",
   "metadata": {},
   "source": [
    "## [Scale Invariant Feature Transform (SIFT)](https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1e53d8",
   "metadata": {},
   "source": [
    "One of the most popular algorithms in image processing is Scale Invariant Feature Transform or SIFT. This algorithm is perfectly suitable for our goal because it detects features that are invariant to image scale and rotation. Moreover, features are local and based on the appearance of the object in certain interesting points. They are also robust to changes in illumination, noise, and minor changes in viewpoint.\n",
    "\n",
    "SIFT was first presented in 2004, by David G. Lowe from the University of British Columbia in the paper, Distinctive Image Features from Scale-Invariant Keypoints, This algorithm is patented, and it is included in the Non-free module in OpenCV. That is why we need to install the older version of OpenCV because SIFT is not included in the new OpenCV library. We can do that with the following code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaa82a5",
   "metadata": {},
   "source": [
    "---\n",
    "### Here's how SIFT works:\n",
    "\n",
    "1. Initialize the SIFT detector by using [cv2.SIFT_create()](https://vovkos.github.io/doxyrest-showcase/opencv/sphinxdoc/page_tutorial_py_sift_intro.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a03188dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sift_Detector = cv.SIFT_create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c024dc20",
   "metadata": {},
   "source": [
    "2. Detect the keypoints and compute descriptors [cv.Sift/DetectandCompute](https://amroamroamro.github.io/mexopencv/matlab/cv.SIFT.detectAndCompute.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c7e3f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sift_keypoints, descriptors = sift_Detector.detectAndCompute(gray.astype(np.uint8), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1a5d24",
   "metadata": {},
   "source": [
    "## What are descriptors?\n",
    "\n",
    "Along with the features we use the term descriptor It can be seen as the way to compare the keypoints by summarizing some characteristics about them. Let’s see some of the feature characteristics.\n",
    "\n",
    "- **Repeatability** or precision means that, when we compare two images, if we find a feature point in the first image, we should better find it in the second image as well.\n",
    "- **Matchability** means that feature should have a description that’s distinctive.\n",
    "- **Locality**, means that the description of the feature is, dependent upon a neighborhood, but not too big a neighborhood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491f8d78",
   "metadata": {},
   "source": [
    "3. Draw the keypoints on the image[cv2.drawKeypoints](https://docs.opencv.org/4.x/d4/d5d/group__features2d__draw.html#ga5d2bafe8c1c45289bc3403a40fb88920):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9e409697",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_sift = cv.drawKeypoints(img, sift_keypoints,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "19d43134",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.imshow(\"sift keypoints\",img_sift)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dabb7b",
   "metadata": {},
   "source": [
    "## ORB (Oriented FAST and Rotated BRIEF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fea943",
   "metadata": {},
   "source": [
    "1. Create ORB object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "12cdb9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORB_object = cv.ORB_create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db968df8",
   "metadata": {},
   "source": [
    "2. detecting the key points in the image using ORB_object.detect() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f264c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints = ORB_object.detect(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74c06f9",
   "metadata": {},
   "source": [
    "3. computing the descriptors for the input image using ORB_object.compute() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8f78d30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints, descriptors = ORB_object.compute(img, keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d488bbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageresult = cv.drawKeypoints(img, keypoints, None, color=(255,0,0), flags=0)\n",
    "#displaying the resulting image as the output on the screen\n",
    "cv.imshow('ORB_image', imageresult)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45d7abf",
   "metadata": {},
   "source": [
    "--- \n",
    "## Feature description and matching techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "826cfaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training and query images\n",
    "query_img = cv.imread('./images/query_image.png') \n",
    "train_img = cv.imread('./images/training_image.jpg') \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a5a0d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the images to grayscale \n",
    "query_img_gray = cv.cvtColor(query_img,cv.COLOR_BGR2GRAY) \n",
    "train_img_gray = cv.cvtColor(train_img, cv.COLOR_BGR2GRAY) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bccb2925",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Initialize the ORB detector algorithm \n",
    "orb = cv.ORB_create() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1d702ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect keypoints (features) cand calculate the descriptors\n",
    "query_keypoints, query_descriptors = orb.detectAndCompute(query_img_gray,None) \n",
    "train_keypoints, train_descriptors = orb.detectAndCompute(train_img_gray,None) \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74742541",
   "metadata": {},
   "source": [
    "## Match Keypoints :\n",
    "1. BF matcher\n",
    "2. FLANN base Matcher "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e23bc0",
   "metadata": {},
   "source": [
    "## BF matcher [cv.BFMatcher()](https://docs.opencv.org/4.5.1/d3/da1/classcv_1_1BFMatcher.html#abe0bb11749b30d97f60d6ade665617bd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1624fa9",
   "metadata": {},
   "source": [
    "Next we create a BFMatcher object with distance measurement `cv.NORM_HAMMING` (since we are using ORB) and `crossCheck` is switched on for better results. Then we use Matcher.match() method to get the best matches in two images. We sort them in ascending order of their distances so that best matches (with low distance) come to front. Then we draw only first 10 matches (Just for sake of visibility. You can increase it as you like)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54036c4c",
   "metadata": {},
   "source": [
    "1. create BF object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ff7726d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match the keypoints\n",
    "matcher = cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "84c6e79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher.match(query_descriptors,train_descriptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810d48c3",
   "metadata": {},
   "source": [
    "2.  match Query with train by using BF.Matcher().match\n",
    "\n",
    "#### What is this Matcher Object?\n",
    "The result of matches = bf.match(des1,des2) line is a list of DMatch objects. This DMatch object has following attributes:\n",
    "\n",
    "- DMatch.distance - Distance between descriptors. The lower, the better it is.\n",
    "- DMatch.trainIdx - Index of the descriptor in train descriptors\n",
    "- DMatch.queryIdx - Index of the descriptor in query descriptors\n",
    "- DMatch.imgIdx - Index of the train image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f590df5c",
   "metadata": {},
   "source": [
    "3. Sort match keypoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5eb23c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort them in the order of their distance.\n",
    "matches = sorted(matches, key = lambda x:x.distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e32ba2",
   "metadata": {},
   "source": [
    "4. draw first 10 matches featues [drawMatches()](https://docs.opencv.org/4.5.1/d4/d5d/group__features2d__draw.html#ga62fbedb5206ab2faf411797e7055c90f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1f1c5389",
   "metadata": {},
   "outputs": [],
   "source": [
    "numberofmatches = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d2587f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw first 10 matches.\n",
    "img3 = cv.drawMatches(query_img,query_keypoints,train_img,train_keypoints,matches[:numberofmatches],None,flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "71d43ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.imshow(\"matched image\", img3)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c8282e",
   "metadata": {},
   "source": [
    "## FLANN base Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0353c6d8",
   "metadata": {},
   "source": [
    "FLANN stands for Fast Library for Approximate Nearest Neighbors. It contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. It works faster than BFMatcher for large datasets. We will see the second example with FLANN based matcher.\n",
    "\n",
    "For FLANN based matcher, we need to pass two dictionaries which specifies the algorithm to be used, its related parameters etc. First one is IndexParams. For various algorithms, the information to be passed is explained in FLANN docs. As a summary, for algorithms like SIFT, SURF etc. you can pass following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7ce32b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate SIFT detector\n",
    "sift = cv.SIFT_create()\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(query_img_gray,None)\n",
    "kp2, des2 = sift.detectAndCompute(train_img_gray,None)\n",
    "# FLANN parameters\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=50) # or pass empty dictionary\n",
    "flann = cv.FlannBasedMatcher(index_params,search_params)\n",
    "matches = flann.knnMatch(des1,des2,k=2)\n",
    "# Need to draw only good matches, so create a mask\n",
    "matchesMask = [[0,0] for i in range(len(matches))]\n",
    "# ratio test as per Lowe's paper\n",
    "for i,(m,n) in enumerate(matches):\n",
    "     if m.distance < 0.7*n.distance:\n",
    "         matchesMask[i]=[1,0]\n",
    "draw_params = dict(matchColor = (0,255,0),\n",
    " singlePointColor = (255,0,0),\n",
    " matchesMask = matchesMask,\n",
    " flags = cv.DrawMatchesFlags_DEFAULT)\n",
    "img3 = cv.drawMatchesKnn(query_img,kp1,train_img,kp2,matches,None,**draw_params)\n",
    "\n",
    "cv.imshow(\"matched image\", img3)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8f32b3",
   "metadata": {},
   "source": [
    "### image Aligment (Basic Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f341b2",
   "metadata": {},
   "source": [
    "At the heart of image alignment techniques is a simple 3×3 matrix called Homography. [The Wikipedia entry](https://en.wikipedia.org/wiki/Homography) for homography can look very scary.\n",
    "\n",
    "Worry you should not because it’s my job to simplify difficult mathematical concepts like homography! great detailed [post](https://learnopencv.com/homography-examples-using-opencv-python-c/). What follows is a shortened version of the explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aa1948",
   "metadata": {},
   "source": [
    "### What is Homography?\n",
    "Two images of a scene are related by a homography under two conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b30dab",
   "metadata": {},
   "source": [
    "1. The two images are that of a plane (e.g. sheet of paper, credit card etc.).\n",
    "2. The two images were acquired by rotating the camera about its optical axis. We take such images while generating panoramas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c1c30b",
   "metadata": {},
   "source": [
    "As mentioned earlier, a homography is nothing but a 3×3 matrix as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f351e26",
   "metadata": {},
   "source": [
    "<img src=\"https://learnopencv.com/wp-content/ql-cache/quicklatex.com-d11346122f20efb9f8535913f20fdfd1_l3.png\" alt=\"drawing\" style=\"width:500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e17f557",
   "metadata": {},
   "source": [
    "Let (x_1,y_1) be a point in the first image and (x_2,y_2)} be the coordinates of the same physical point in the second image. Then, the Homography H relates them in the following way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324971ef",
   "metadata": {},
   "source": [
    "<img src=\"https://learnopencv.com/wp-content/ql-cache/quicklatex.com-c37a548be445fbbbdba716ff3cac7281_l3.png\n",
    "\" alt=\"drawing\" style=\"width:500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78467d5b",
   "metadata": {},
   "source": [
    "If we knew the homography, we could apply it to all the pixels of one image to obtain a warped image that is aligned with the second image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1d01e26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_image = cv.imread('./images/form.jpg')\n",
    "train_image = cv.imread('./images/scanned-form.jpg')\n",
    "query_imggray = cv.cvtColor(query_image , cv.COLOR_BGR2GRAY)\n",
    "train_imggray= cv.cvtColor(train_image , cv.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "16fa1368",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 500\n",
    "GOOD_MATCH_PERCENT = 0.15\n",
    "# Detect ORB features and compute descriptors.\n",
    "orb = cv.ORB_create(MAX_FEATURES)\n",
    "keypoints1, descriptors1 = orb.detectAndCompute(train_imggray, None)\n",
    "keypoints2, descriptors2 = orb.detectAndCompute(query_imggray, None)\n",
    " \n",
    "  # Match features.\n",
    "matcher = cv.DescriptorMatcher_create(cv.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)\n",
    "matches = matcher.match(descriptors1, descriptors2, None)\n",
    " \n",
    "  # Sort matches by score\n",
    "matched = sorted(matches , key=lambda x: x.distance, reverse=False)\n",
    " \n",
    "  # Remove not so good matches\n",
    "numGoodMatches = int(len(matched) * GOOD_MATCH_PERCENT)\n",
    "matches = matched[:numGoodMatches]\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9871a755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract location of good matches\n",
    "points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    " \n",
    "for i, match in enumerate(matches):\n",
    "    points1[i, :] = keypoints1[match.queryIdx].pt\n",
    "    points2[i, :] = keypoints2[match.trainIdx].pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5b65929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find homography\n",
    "h, mask = cv.findHomography(points1, points2, cv.RANSAC)\n",
    " \n",
    "# Use homography\n",
    "height, width, channels = query_image.shape\n",
    "im1Reg = cv.warpPerspective(train_image, h, (width, height))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e5a6f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.imshow(\"query\", train_image)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8eddc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving aligned image :  aligned.jpg\n"
     ]
    }
   ],
   "source": [
    "# Write aligned image to disk. \n",
    "outFilename = \"aligned.jpg\"\n",
    "print(\"Saving aligned image : \", outFilename); \n",
    "cv.imshow(outFilename, im1Reg)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ab25d9",
   "metadata": {},
   "source": [
    "readList : \n",
    "\n",
    "1. [read baics blog on Featue descrptor and matching](https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_table_of_contents_feature2d/py_table_of_contents_feature2d.html)\n",
    "2. [read blog in image staching ](https://pyimagesearch.com/2016/01/11/opencv-panorama-stitching/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc61505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
