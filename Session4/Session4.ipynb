{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db18a520",
   "metadata": {},
   "source": [
    "# Session 4\n",
    "1. Object Detection \n",
    "    - Haar cascade\n",
    "    - Yolo\n",
    "2. Object tracking techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271fdc98",
   "metadata": {},
   "source": [
    "# Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a5e4b1",
   "metadata": {},
   "source": [
    "## What is the Object Detection? \n",
    "<img src = \"https://deeplobe.ai/wp-content/uploads/2023/06/Object-detection-Real-world-applications-and-benefits.png\" alt=\"drawing\" style=\"width:500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc96863",
   "metadata": {},
   "source": [
    "## Object Detection\n",
    "1. **Recognation** \n",
    "2. **loclization** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221211ea",
   "metadata": {},
   "source": [
    "## Applications of Object Detection \n",
    "1. Object Tracking\n",
    "2. Self- Driving car\n",
    "3. Anamoly Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647acd16",
   "metadata": {},
   "source": [
    "\n",
    "## Haar Classifier - Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8776627c",
   "metadata": {},
   "source": [
    "### Haar Features in images \n",
    "    - Efficient Calculation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1618ab",
   "metadata": {},
   "source": [
    "###  Traning the Classifier with images\n",
    "    - Both positive and negative samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005b6bc1",
   "metadata": {},
   "source": [
    "### Selects features during training to create strong classifiers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c3c52b",
   "metadata": {},
   "source": [
    "### Weak and Stronf Classifiers \n",
    "    - Adaboost to make it more efficient and having less features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6213bf",
   "metadata": {},
   "source": [
    "### Haar - Features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff2f834",
   "metadata": {},
   "source": [
    "<img src= \"https://miro.medium.com/v2/resize:fit:1400/1*JhFCP1CjF7fRYt9pLldMsw.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecebf9d",
   "metadata": {},
   "source": [
    "- Each feature is a single value obtained by subtracting sum of pixels under the white rectangle from sum of pixels under the black rectangle \n",
    "- Specific feature depends on wgat we want to detect \n",
    "- How Can we select the best features \n",
    "     - Adaboost- finds the best features ? \n",
    "     - is used to find the best features and reduce them "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950d21d4",
   "metadata": {},
   "source": [
    "<img src = \"https://miro.medium.com/v2/resize:fit:828/format:webp/1*MDkpzf_3JidDLHhR09yawA.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbbe25b",
   "metadata": {},
   "source": [
    "<img src = \"https://miro.medium.com/v2/resize:fit:828/format:webp/1*gl4JHntNPHQt1G7txpiRMA.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03175441",
   "metadata": {},
   "source": [
    "Haar features are similar to these convolution kernels which are used to detect the presence of that feature in the given image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c6ff2c",
   "metadata": {},
   "source": [
    "#### Detect edges using convolution kernels:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd5049f",
   "metadata": {},
   "source": [
    "Given an input image and convolution kernel, we place kernel to a corner and do convolution multiplication shifting the kernels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcddad4",
   "metadata": {},
   "source": [
    "<img src = \"https://miro.medium.com/v2/resize:fit:640/0*Ms94ZOyysSZ7c6A8.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b25b48a",
   "metadata": {},
   "source": [
    "This method is used to detect different types of edges using different kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a669bfeb",
   "metadata": {},
   "source": [
    "A Haar-Feature is just like a kernel in CNN, except that in a CNN, the values of the kernel are determined by training, while a Haar-Feature is manually determined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd26b9c",
   "metadata": {},
   "source": [
    "<img src = \"https://miro.medium.com/v2/resize:fit:640/format:webp/0*g_FpO-ZYVFWiedDF.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11e09ad",
   "metadata": {},
   "source": [
    "Here are some Haar-Features. The first two are “edge features”, used to detect edges. The third is a “line feature”, while the fourth is a “four rectangle feature”, most likely used to detect a slanted line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abc39d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90a95301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Haar cascade classifier\n",
    "face_cascade = cv.CascadeClassifier('./HaarCasscade/haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55580712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "image = cv.imread('img/Faces.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5012f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image to grayscale\n",
    "gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5066876c",
   "metadata": {},
   "source": [
    "[detectMultiScale(scaleFactor , minNeighbors , minSize)](https://docs.opencv.org/3.4/d1/de5/classcv_1_1CascadeClassifier.html)\n",
    "\n",
    "Parameters in detectMultiScale\n",
    "\n",
    "- `scaleFactor` – This tells how much the object’s size is reduced in each image.\n",
    "- `minNeighbors` – This parameter tells how many neighbours each rectangle candidate should consider.\n",
    "- `minSize` — This signifies the minimum possible size of an object to be detected. An object smaller than minSize would be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d57c31",
   "metadata": {},
   "source": [
    "Tuning Cascade Classifiers\n",
    "ourClassifier.detectMultiScale(input image, Scale Factor , Min Neighbors)\n",
    "\n",
    "    - Scale Factor\n",
    "Specifies how much we reduce the image size each time we scale. E.g. in face detection we typically use 1.3. This means we reduce the image by 30% each time it’s scaled. Smaller values, like 1.05 will take longer to compute, but will increase the rate of detection.\n",
    "\n",
    "    - Min Neighbors\n",
    "Specifies the number of neighbors each potential window should have in order to consider it a positive detection. Typically set between 3-6. It acts as sensitivity setting, low values will sometimes detect multiples faces over a single face. High values will ensure less false positives, but you may miss some faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0b01d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform face detection\n",
    "faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5, minSize=(30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63a24f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw bounding boxes around the detected faces\n",
    "for (x, y, w, h) in faces:\n",
    "    cv.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aed5caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image with bounding boxes\n",
    "cv.imshow('Face Detection', image)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b576ab6f",
   "metadata": {},
   "source": [
    "# Yolo (You only look once)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3609165a",
   "metadata": {},
   "source": [
    "https://www.datacamp.com/blog/yolo-object-detection-explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "789222c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_layers(net):\n",
    "    \n",
    "    layer_names = net.getLayerNames()\n",
    "    try:\n",
    "        output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    except:\n",
    "        output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    return output_layers\n",
    "\n",
    "\n",
    "def draw_prediction(img, class_id, confidence, x, y, x_plus_w, y_plus_h):\n",
    "\n",
    "    label = str(classes[class_id])\n",
    "\n",
    "    color = COLORS[class_id]\n",
    "\n",
    "    cv.rectangle(img, (x,y), (x_plus_w,y_plus_h), color, 2)\n",
    "\n",
    "    cv.putText(img, label, (x-10,y-10), cv.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2d9dd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv.imread(\"img/dog.jpg\")\n",
    "\n",
    "Width = image.shape[1]\n",
    "Height = image.shape[0]\n",
    "scale = 0.00392\n",
    "\n",
    "classes = None\n",
    "\n",
    "with open(\"Yolo/yolov3.txt\", 'r') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "net = cv.dnn.readNet(\"Yolo/yolov3.weights\", \"Yolo/yolov3.cfg\")\n",
    "\n",
    "blob = cv.dnn.blobFromImage(image, scale, (416,416), (0,0,0), True, crop=False)\n",
    "\n",
    "net.setInput(blob)\n",
    "\n",
    "outs = net.forward(get_output_layers(net))\n",
    "\n",
    "class_ids = []\n",
    "confidences = []\n",
    "boxes = []\n",
    "conf_threshold = 0.5\n",
    "nms_threshold = 0.4\n",
    "\n",
    "\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.5:\n",
    "            center_x = int(detection[0] * Width)\n",
    "            center_y = int(detection[1] * Height)\n",
    "            w = int(detection[2] * Width)\n",
    "            h = int(detection[3] * Height)\n",
    "            x = center_x - w / 2\n",
    "            y = center_y - h / 2\n",
    "            class_ids.append(class_id)\n",
    "            confidences.append(float(confidence))\n",
    "            boxes.append([x, y, w, h])\n",
    "\n",
    "\n",
    "indices = cv.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "\n",
    "for i in indices:\n",
    "    try:\n",
    "        box = boxes[i]\n",
    "    except:\n",
    "        i = i[0]\n",
    "        box = boxes[i]\n",
    "    \n",
    "    x = box[0]\n",
    "    y = box[1]\n",
    "    w = box[2]\n",
    "    h = box[3]\n",
    "    draw_prediction(image, class_ids[i], confidences[i], round(x), round(y), round(x+w), round(y+h))\n",
    "\n",
    "cv.imshow(\"object detection\", image)\n",
    "cv.waitKey()\n",
    "    \n",
    "cv.imwrite(\"object-detection.jpg\", image)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6fdf89",
   "metadata": {},
   "source": [
    "## Object tracking techniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2b44220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import statistics\n",
    "cap=cv2.VideoCapture(\"Videos/video.mp4\")\n",
    "f=[]\n",
    "success, img = cap.read()\n",
    "tracker=cv2.legacy.TrackerMOSSE_create()\n",
    "bbox=cv2.selectROI(\"output\",img,False)\n",
    "tracker.init(img ,bbox)\n",
    "def draw(img,bbox):\n",
    "    x,y,w,h=int(bbox[0]),int(bbox[1]),int(bbox[2]),int(bbox[3])\n",
    "    cv2.rectangle(img,(x,y),((x+w),(y+h)),(0,255,255),3,1)\n",
    "while True:\n",
    "    success,img=cap.read()\n",
    "    sucess,bbox=tracker.update(img)\n",
    "    if sucess:\n",
    "        draw(img,bbox)\n",
    "    else:\n",
    "        cv2.putText(img,\"lost\",(75,55),cv2.FONT_HERSHEY_SIMPLEX,0.7,(255,255,0),2)\n",
    "    cv2.imshow(\"output\",img)\n",
    "    k=cv2.waitKey(1)\n",
    "    if k==27:\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cb41bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = cv2.TrackerKCF_create()\n",
    "video = cv2.VideoCapture(\"Videos/video.mp4\")\n",
    "ok,frame=video.read()\n",
    "bbox=cv2.selectROI(\"output\",frame,False)\n",
    "ok = tracker.init(frame,bbox)\n",
    "while True:\n",
    "   ok,frame=video.read()\n",
    "   if not ok:\n",
    "        break\n",
    "   ok,bbox=tracker.update(frame)\n",
    "   if ok:\n",
    "        (x,y,w,h)=[int(v) for v in bbox]\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),2,1)\n",
    "   else:\n",
    "        cv2.putText(frame,'Error',(100,0),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),2)\n",
    "   cv2.imshow('Tracking',frame)\n",
    "   if cv2.waitKey(1) & 0XFF==27:\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6837f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_classifier = cv2.CascadeClassifier('./HaarCasscade/haarcascade_frontalface_default.xml')\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "435e5a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_bounding_box(vid):\n",
    "    gray_image = cv2.cvtColor(vid, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray_image, 1.1, 5, minSize=(40, 40))\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(vid, (x, y), (x + w, y + h), (0, 255, 0), 4)\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7b24300",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\n",
    "    result, video_frame = video_capture.read()  # read frames from the video\n",
    "    if result is False:\n",
    "        break  # terminate the loop if the frame is not read successfully\n",
    "\n",
    "    faces = detect_bounding_box(\n",
    "        video_frame\n",
    "    )  # apply the function we created to the video frame\n",
    "\n",
    "    cv2.imshow(\n",
    "        \"My Face Detection Project\", video_frame\n",
    "    )  # display the processed frame in a window named \"My Face Detection Project\"\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7ba874",
   "metadata": {},
   "source": [
    "## Good REF :\n",
    "1. https://www.datacamp.com/blog/yolo-object-detection-explained\n",
    "2. https://www.datacamp.com/tutorial/face-detection-python-opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e9c4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
